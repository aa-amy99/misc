{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import pyspark \nfrom pyspark import SparkContext", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "from pyspark.sql import SQLContext\nfrom pyspark.conf import SparkConf\nfrom pyspark.sql.session import SparkSession", "outputs": [], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "#sc =. SparkContext()\nsc", "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "<SparkContext master=yarn appName=PySparkShell>", "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://spark-etl-m.us-east1-b.c.bigdata-etl-332614.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.3.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        "}, "metadata": {}}], "metadata": {}}, {"source": "# Extract", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "spark = SQLContext(sc)", "outputs": [], "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "flights_data = spark.read.json(\"gs://amy_data_etl/fight-data/2019-05-06.json\") #df", "outputs": [], "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "flights_data.registerTempTable(\"flights_data\")#to run sql", "outputs": [], "metadata": {}}, {"source": "## Understand data", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "spark.sql(\"select * from flights_data limit 10\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-------+-------------+------------+---------------+--------------+-------------------+--------+-----------+----------+------+--------------+\n|airline_code|airtime|arrival_delay|arrival_time|departure_delay|departure_time|destination_airport|distance|flight_date|flight_num|    id|source_airport|\n+------------+-------+-------------+------------+---------------+--------------+-------------------+--------+-----------+----------+------+--------------+\n|       20304|     88|           13|        1421|              8|          1228|                PHX|     598| 2019-05-06|      2916|200000|           MRY|\n|       20304|     69|           -3|        1721|             -6|          1454|                GJT|     438| 2019-05-06|      2916|200001|           PHX|\n|       20304|     62|            2|        2203|            -10|          2128|                PHX|     347| 2019-05-06|      2917|200002|           ELP|\n|       20304|     67|          -15|        2039|             -8|          1917|                PHX|     369| 2019-05-06|      2918|200003|           BUR|\n|       20304|     46|          -10|        2347|             -7|          2143|                ABQ|     328| 2019-05-06|      2918|200004|           PHX|\n|       20304|     61|            2|        2232|             -1|          2114|                PHX|     370| 2019-05-06|      2576|200005|           LAX|\n|       20304|    106|          -18|        1817|            -11|          1614|                SBA|     784| 2019-05-06|      3494|200006|           PDX|\n|       20304|    110|          -13|        1620|             -8|          1412|                SEA|     748| 2019-05-06|      3495|200007|           FAT|\n|       20304|     93|          -15|        1327|             -7|          1138|                FAT|     748| 2019-05-06|      3496|200008|           SEA|\n|       20304|    152|           -7|         948|             -8|           752|                SEA|    1068| 2019-05-06|      3497|200009|           COS|\n+------------+-------+-------------+------------+---------------+--------------+-------------------+--------+-----------+----------+------+--------------+\n\n"}], "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "spark.sql(\"select distinct flight_num from flights_data\" ).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+\n|flight_num|\n+----------+\n|      5409|\n|      1697|\n|       474|\n|      1806|\n|      1950|\n|       964|\n|        29|\n|      3764|\n|      2250|\n|      3506|\n|        26|\n|      1677|\n|      2214|\n|      2453|\n|      2509|\n|      2529|\n|      2040|\n|      4894|\n|      5385|\n|      2927|\n+----------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "spark.sql(\"select count(*) from flights_data\" ).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+\n|count(1)|\n+--------+\n|  100001|\n+--------+\n\n"}], "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "spark.sql(\"select max(distance) from flights_data limit 10\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------+\n|max(distance)|\n+-------------+\n|         4983|\n+-------------+\n\n"}], "metadata": {}}, {"source": "# Transform", "cell_type": "markdown", "metadata": {}}, {"source": "## 1. find average delay by fight number and date", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "qry = \"\"\"\n        select \n            flight_date , \n            round(avg(arrival_delay),2) as avg_arrival_delay,\n            round(avg(departure_delay),2) as avg_departure_delay,\n            flight_num \n        from \n            flights_data \n        group by \n            flight_num , \n            flight_date \n      \"\"\"", "outputs": [], "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "avg_delays_by_flight_nums = spark.sql(qry)", "outputs": [], "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "avg_delays_by_flight_nums.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+-----------------+-------------------+----------+\n|flight_date|avg_arrival_delay|avg_departure_delay|flight_num|\n+-----------+-----------------+-------------------+----------+\n| 2019-05-06|              5.5|               6.17|      6430|\n| 2019-05-06|            -2.48|                3.3|      4480|\n| 2019-05-06|           -13.29|              -4.86|      4757|\n| 2019-05-06|            11.76|              16.52|      5323|\n| 2019-05-06|             4.17|               6.11|      5380|\n| 2019-05-06|             5.64|               8.57|      3705|\n| 2019-05-06|             6.27|              13.47|      1611|\n| 2019-05-06|            -7.29|              -2.66|      1732|\n| 2019-05-06|            14.33|               20.0|      1742|\n| 2019-05-06|            -1.79|               7.69|       429|\n| 2019-05-06|             -3.0|               4.67|       453|\n| 2019-05-06|            -5.22|              -1.17|       789|\n| 2019-05-06|            -1.56|               1.81|      1138|\n| 2019-05-06|             6.95|              10.64|      1527|\n| 2019-05-06|             1.67|               0.86|      1797|\n| 2019-05-06|            -5.44|              -0.56|      3688|\n| 2019-05-06|             6.31|               3.38|      2623|\n| 2019-05-06|            -1.58|               5.45|        67|\n| 2019-05-06|             3.09|               6.94|      1307|\n| 2019-05-06|             7.75|               5.33|      1337|\n+-----------+-----------------+-------------------+----------+\nonly showing top 20 rows\n\n"}], "metadata": {}}, {"source": "## 2. find average delay by distance (1196 values, so you need to bucket them)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "spark.sql(\"select count(distinct distance) from flights_data\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------------------+\n|count(DISTINCT distance)|\n+------------------------+\n|                    1196|\n+------------------------+\n\n"}], "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "spark.sql(\"select max(distance) from flights_data limit 10\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------------+\n|max(distance)|\n+-------------+\n|         4983|\n+-------------+\n\n"}], "metadata": {}}, {"source": "## 2.1 bucket them and save it to new coloumn", "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": "qry = \"\"\"\n        select \n            *,\n            case \n                when distance between 0 and 500 then 1 \n                when distance between 501 and 1000 then 2\n                when distance between 1001 and 2000 then 3\n                when distance between 2001 and 3000 then 4 \n                when distance between 3001 and 4000 then 5 \n                when distance between 4001 and 5000 then 6 \n            END distance_category \n        from \n            flights_data \n        \"\"\"", "outputs": [], "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "spark.sql(qry).show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-------+-------------+------------+---------------+--------------+-------------------+--------+-----------+----------+------+--------------+-----------------+\n|airline_code|airtime|arrival_delay|arrival_time|departure_delay|departure_time|destination_airport|distance|flight_date|flight_num|    id|source_airport|distance_category|\n+------------+-------+-------------+------------+---------------+--------------+-------------------+--------+-----------+----------+------+--------------+-----------------+\n|       20304|     88|           13|        1421|              8|          1228|                PHX|     598| 2019-05-06|      2916|200000|           MRY|                2|\n|       20304|     69|           -3|        1721|             -6|          1454|                GJT|     438| 2019-05-06|      2916|200001|           PHX|                1|\n|       20304|     62|            2|        2203|            -10|          2128|                PHX|     347| 2019-05-06|      2917|200002|           ELP|                1|\n|       20304|     67|          -15|        2039|             -8|          1917|                PHX|     369| 2019-05-06|      2918|200003|           BUR|                1|\n|       20304|     46|          -10|        2347|             -7|          2143|                ABQ|     328| 2019-05-06|      2918|200004|           PHX|                1|\n+------------+-------+-------------+------------+---------------+--------------+-------------------+--------+-----------+----------+------+--------------+-----------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"source": "## 2.2 add distance_category to original dataset and register for table", "cell_type": "markdown", "metadata": {}}, {"execution_count": 40, "cell_type": "code", "source": "# add distance_category to original dataset\nflights_data = spark.sql(qry)", "outputs": [], "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": "flights_data.registerTempTable(\"flights_data\")", "outputs": [], "metadata": {}}, {"source": "## 2.3 calculate average", "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "# calculate average\nqry = \"\"\"\n        select \n            flight_date , \n            round(avg(arrival_delay),2) as avg_arrival_delay,\n            round(avg(departure_delay),2) as avg_departure_delay,\n            distance_category \n        from \n            flights_data \n        group by \n            distance_category , \n            flight_date \n      \"\"\"\n\n\n#spark.sql(qry).show()\navg_delays_by_distance_category = spark.sql(qry)", "outputs": [], "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": "avg_delays_by_distance_category.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+-----------------+-------------------+-----------------+\n|flight_date|avg_arrival_delay|avg_departure_delay|distance_category|\n+-----------+-----------------+-------------------+-----------------+\n| 2019-05-06|            -5.42|               3.96|                6|\n| 2019-05-06|            -0.02|               7.56|                4|\n| 2019-05-06|             6.72|               9.91|                2|\n| 2019-05-06|             6.16|               8.11|                1|\n| 2019-05-06|            -4.92|               5.52|                5|\n| 2019-05-06|              5.1|               9.75|                3|\n+-----------+-----------------+-------------------+-----------------+\n\n"}], "metadata": {}}, {"source": "# Load to Bucket with timestamp", "cell_type": "markdown", "metadata": {}}, {"execution_count": 45, "cell_type": "code", "source": "from datetime import date \ncurrent_date = date.today()\nfile_name = str(current_date)", "outputs": [], "metadata": {}}, {"execution_count": 53, "cell_type": "code", "source": "bucket_name = \"gs://amy_data_etl\"\noutput_flight_nums = bucket_name+\"/flights_data_output/\"+file_name+\"_flight_nums\"\noutput_distance_category = bucket_name+\"/flights_data_output/\"+file_name+\"_distance_category\"\n\navg_delays_by_flight_nums.coalesce(1).write.format(\"json\").save(output_flight_nums)\navg_delays_by_distance_category.coalesce(1).write.format(\"json\").save(output_distance_category)\n", "outputs": [], "metadata": {}}, {"execution_count": 48, "cell_type": "code", "source": "output_flight_nums", "outputs": [{"execution_count": 48, "output_type": "execute_result", "data": {"text/plain": "'gs://amy_data_etl/flights_data_output/2021-11-20_flight_nums'"}, "metadata": {}}], "metadata": {}}, {"execution_count": 49, "cell_type": "code", "source": "output_distance_category", "outputs": [{"execution_count": 49, "output_type": "execute_result", "data": {"text/plain": "'gs://amy_data_etl/flights_data_output/2021-11-20_distance_category'"}, "metadata": {}}], "metadata": {}}, {"source": "# Create table and Load outputs from gs to  Bigquery", "cell_type": "markdown", "metadata": {}}, {"source": "## path: /Users/amy_a/Desktop/gcp-data-engineering-master/bigquery-sparksql-batch-etl/spark-sql", "cell_type": "markdown", "metadata": {}}, {"source": "### 1. create_tables.sh\n### 2. load_json_partitioned.sh", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}